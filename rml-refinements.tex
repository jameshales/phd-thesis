\section{Refinements}\label{rml-refinements}

Refinements are relations over Kripke models that, in epistemic settings, can be seen as indicating that one Kripke model is the result of an epistemic update of another.
In Section~\ref{ml} we introduced bisimulations, which are relations over Kripke models that indicate that one Kripke model is modally equivalent to another.
Recall that a bisimulation is a relation between two Kripke models that must satisfy the conditions {\bf atoms}, {\bf forth}, and {\bf back} for every propositional atom and every agent.
A refinement is a generalisation of a bisimulation that allows {\bf forth} to be relaxed for a given set of agents.
A refinement is also the reverse of a simulation, which relaxes {\bf back} for all agents.

\begin{definition}[Refinements]\label{refinements}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kModelAndTuple$ and $\kModelAndTupleP$ be Kripke models.
A non-empty relation $\refinement \subseteq \kStates \times \kStatesP$ is a {\em $\agentsB$-refinement from $\kModel$ to $\kModelP$} if and only if for every $\atomP \in \atoms$, $\agentA \in \agents$, $\agentC \in \agents \setminus \agentsB$ and $(\kStateS, \kStateSP) \in \refinement$ the conditions {\bf atoms-$\atomP$}, {\bf forth-$\agentC$} and {\bf back-$\agentA$} holds:

\paragraph{atoms-$\atomP$}
$\kStateS \in \kValuation(\atomP)$ if and only if $\kStateSP \in \kValuationP(\atomP)$.

\paragraph{forth-$\agentC$}
For every $\kStateT \in \kSuccessorsC{\kStateS}$ there exists $\kStateTP \in \kSuccessorsPC{\kStateSP}$ such that $(\kStateT, \kStateTP) \in \refinement$.

\paragraph{back-$\agentA$}
For every $\kStateTP \in \kSuccessorsPA{\kStateSP}$ there exists $\kStateT \in \kSuccessorsA{\kStateS}$ such that $(\kStateT, \kStateTP) \in \refinement$.

If there exists a $\agentsB$-refinement $\refinement$ from $\kModel$ to $\kModelP$ such that $(\kStateS, \kStateSP) \in \refinement$ then we say that $\kPModelP{\kStateSP}$ is a $\agentsB$-refinement of $\kPModel{\kStateS}$ and we denote this by $\kPModelP{\kStateSP} \refinesBs \kPModel{\kStateS}$ or equivalently $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
\end{definition}

We call an $\agents$-refinement simply a {\em refinement} and we write $\kPModelP{\kStateSP} \refines \kPModel{\kStateS}$ or equivalently $\kPModel{\kStateS} \simulates \kPModelP{\kStateSP}$.
We call an $\{\agentA\}$-refinement simply an {\em $\agentA$-refinement} and we write $\kPModelP{\kStateSP} \refinesA \kPModel{\kStateS}$ or equivalently $\kPModel{\kStateS} \simulatesA \kPModelP{\kStateSP}$.

We note that we use the term ``refinement'' both to refer to a relation between two Kripke models, and to refer to a Kripke model that is related to another Kripke model through such a relation.
It should be clear from context which definition we are using.
In informal settings we often use the term ``refinement'' to refer to the general concept of $\agentsB$-refinements, rather than to refer only to $\agents$-refinements.

We provide some examples of refinements.

\begin{example}\label{example-refinement-pal}
Let $\kPModelAndTuple{\kStateS}$ and $\kPModelAndTupleP{\kStateS}$ be pointed Kripke models where:
\begin{eqnarray*}
    \kStates &=& \{\kStateS, \kStateT, \kStateU, \kStateV\}\\
    \kAccessibilityA &=& \{(\kStateS, \kStateS), (\kStateS, \kStateT), (\kStateT, \kStateS), (\kStateT, \kStateT), (\kStateU, \kStateU), (\kStateU, \kStateV), (\kStateV, \kStateU), (\kStateV, \kStateV)\}\\
    \kAccessibilityB &=& \{(\kStateS, \kStateS), (\kStateS, \kStateU), (\kStateU, \kStateS), (\kStateU, \kStateU), (\kStateT, \kStateT), (\kStateT, \kStateV), (\kStateV, \kStateT), (\kStateV, \kStateV)\}\\
    \kValuation(\atomP) &=& \{\kStateS, \kStateT\}\\
    \kValuation(\atomQ) &=& \{\kStateS, \kStateU\}
\end{eqnarray*}
and:
\begin{eqnarray*}
    \kStatesP &=& \{\kStateS, \kStateT\}\\
    \kAccessibilityPA &=& \{(\kStateS, \kStateS), (\kStateS, \kStateT), (\kStateT, \kStateS), (\kStateT, \kStateT)\}\\
    \kAccessibilityPB &=& \{(\kStateS, \kStateS), (\kStateT, \kStateT)\}\\
    \kValuationP(\atomP) &=& \{\kStateS, \kStateT\}\\
    \kValuationP(\atomQ) &=& \{\kStateS\}
\end{eqnarray*}

\begin{figure}
    \caption{An example of a Kripke model and refinement.}\label{example-refinement-pal-figure}
    \centering
    \begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=7em,thick]

      \node[label=above right:{$\kStateS$}] (s) {\underline{$\{\atomP, \atomQ\}$}};
      \node[label=below right:{$\kStateT$}] (t) [below of=s] {$\{\atomP\}$};
      \node[label=above left:{$\kStateU$}] (u) [left of=s] {$\{\atomQ\}$};
      \node[label=below left:{$\kStateV$}] (v) [left of=t] {$\{\}$};
      \node[label=above:{$\kPModel{\kStateS}$},draw=black, fit=(s) (t) (u) (v), inner sep=3em, dashed, rounded corners] {};

      \path[every node/.style={font=\sffamily\small},->]
        (s) edge [loop above] node {$\agentA, \agentB$} (s)
        (t) edge [loop below] node {$\agentA, \agentB$} (t)
        (u) edge [loop above] node {$\agentA, \agentB$} (u)
        (v) edge [loop below] node {$\agentA, \agentB$} (v)
        (s) [<->] edge node {$\agentA$} (t)
        (u) [swap] edge node {$\agentA$} (v)
        (s) edge node {$\agentB$} (u)
        (t) [swap] edge node {$\agentB$} (v);

      \node[label=above left:{$\kStateS$}] (s') [right=9em of s]{\underline{$\{\atomP, \atomQ\}$}};
      \node[label=below left:{$\kStateT$}] (t') [below of=s'] {$\{\atomP\}$};
      \node[label=above:{$\restrict{\kPModel{\kStateS}}{\knowsA \atomP}$},draw=black, fit=(s') (t'), inner sep=3em, dashed, rounded corners] {};

      \path[every node/.style={font=\sffamily\small},->]
        (s') edge [loop above] node {$\agentA, \agentB$} (s')
        (t') edge [loop below] node {$\agentA, \agentB$} (t')
        (s') [<->] edge node {$\agentA$} (t');
      
      \path[every node/.style={font=\sffamily\small},->,dotted]
        (s) edge node {$\simulatesB$} (s')
        (t) edge node {$\simulatesB$} (t');
    \end{tikzpicture}
\end{figure}

The Kripke models $\kPModel{\kStateS}$ and $\kPModelP{\kStateS}$ are shown in Figure~\ref{example-refinement-pal-figure}.
We note that $\kPModel{\kStateS}$ and $\kPModelP{\kStateS}$ are essentially the same as (are isomorphic to) the Kripke models from Example~\ref{example-pal}.
In Example~\ref{example-pal} we showed that $\kPModelP{\kStateS}$ is the result of executing an action model on $\kPModel{\kStateS}$.

We note that $\kPModel{\kStateS} \simulatesB \kPModelP{\kStateS}$ via the $\agentB$-refinement $\refinement = \{(\kStateS, \kStateS), (\kStateT, \kStateT)\}$.
It is simple to check that {\bf atoms-$\atomP$}, {\bf atoms-$\atomQ$}, {\bf forth-$\agentA$}, {\bf back-$\agentA$} and {\bf back-$\agentB$} hold for $\refinement$.

We also show that $\kPModelP{\kStateS} \not\simulatesB \kPModel{\kStateS}$ by the following argument.
Suppose that $\refinement' \subseteq \kStatesP \times \kStates$ is a relation from $\kPModelP{\kStateS}$ to $\kPModel{\kStateS}$ such that $\refinement'$ satisfies {\bf atoms-$\atomP$} and $(\kStateS, \kStateS) \in \refinement'$.
As $\kStateU \in \kSuccessorsB{\kStateS}$ then in order for $\refinement'$ to satisfy {\bf back-$\agentB$} there must exist some $\kStateX \in \kSuccessorsPB{\kStateS}$ such that $(\kStateX, \kStateU) \in \refinement'$.
However $\kStateU \notin \kValuation(\atomP)$ but $\kStateS, \kStateT \in \kValuationP(\atomP)$ so, as $\refinement'$ satisfies {\bf atoms-$\atomP$}, we must have $(\kStateS, \kStateU) \notin \refinement'$ and $(\kStateT, \kStateU) \notin \refinement'$.
Then $\refinement'$ does not satisfy {\bf back-$\agentB$} and is not a $\agentB$-refinement.
Therefore there does not exist a $\agentB$-refinement from $\kPModelP{\kStateS}$ to $\kPModel{\kStateS}$ and $\kPModelP{\kStateS} \not\simulatesB \kPModel{\kStateS}$.
\end{example}

\vfill

\begin{example}\label{example-refinement-aml}
Let $\kPModelAndTuple{\kStateS}$ and $\kPModelAndTupleP{\kStateS}$ be Kripke models where:
\begin{eqnarray*}
    \kStates &=& \{\kStateS, \kStateT\}\\
    \kAccessibilityA &=& \{(\kStateS, \kStateS), (\kStateT, \kStateT)\}\\
    \kAccessibilityB &=& \{(\kStateS, \kStateS), (\kStateS, \kStateT), (\kStateT, \kStateS), (\kStateT, \kStateT)\}\\
    \kValuation(\atomP) &=& \{\kStateS\}\\
\end{eqnarray*}
and:
\begin{eqnarray*}
    \kStatesP &=& \{\kStateS, \kStateT, \kStateU\}\\
    \kAccessibilityPA &=& \{(\kStateS, \kStateS), (\kStateT, \kStateT), (\kStateS, \kStateU), (\kStateU, \kStateS), (\kStateU, \kStateU)\}\\
    \kAccessibilityPB &=& \{(\kStateS, \kStateS), (\kStateS, \kStateT), (\kStateT, \kStateS), (\kStateT, \kStateT), (\kStateU, \kStateU)\}\\
    \kValuationP(\atomP) &=& \{\kStateS, \kStateU\}
\end{eqnarray*}

\begin{figure}
    \caption{An example of two Kripke models that are refinements of each other.}\label{example-refinement-aml-figure}
    \centering
    \begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=7em,thick]

      \node[label=above right:{$\kStateS$}] (s') {\underline{$\{\atomP\}$}};
      \node[label=above right:{$\kStateT$}] (t') [above of=s'] {$\{\}$};
      \node[label=above:{$\kPModel{\kStateS}$},draw=black, fit=(s') (t'), inner sep=3.5em, dashed, rounded corners] {};

      \path[every node/.style={font=\sffamily\small},->]
        (s') edge [loop left] node {$\agentA, \agentB$} (s')
        (t') edge [loop left] node {$\agentA, \agentB$} (t')
        (s') [<->] edge node {$\agentB$} (t');

      \node[label=above left:{$\kStateS$}] (s) [right=11em of s] {\underline{$\{\atomP\}$}};
      \node[label=above left:{$\kStateT$}] (t) [above of=s] {$\{\}$};
      \node[label=above left:{$\kStateU$}] (u) [below of=s] {$\{\atomP\}$};
      \node[label=above:{$\kPModelP{\kStateS}$},draw=black, fit=(s) (t) (u), inner sep=3.5em, dashed, rounded corners] {};

      \path[every node/.style={font=\sffamily\small},->]
        (s) edge [loop right] node {$\agentA, \agentB$} (s)
        (t) edge [loop right] node {$\agentA, \agentB$} (t)
        (u) edge [loop right] node {$\agentA, \agentB$} (u)
        (s) [<->] edge node {$\agentB$} (t)
        (s) edge node {$\agentA$} (u);
      
      \path[every node/.style={font=\sffamily\small},<->,dotted]
        (s') edge node {$\simulatesA$} (s)
        (t') edge node {$\simulatesA$} (t)
        (s') edge [swap,->,bend left] node {$\simulatesA$} (u);
    \end{tikzpicture}
\end{figure}

The Kripke models $\kPModel{\kStateS}$ and $\kPModelP{\kStateS}$ are shown in Figure~\ref{example-refinement-aml-figure}.
We note that $\kPModel{\kStateS}$ and $\kPModelP{\kStateS}$ are essentially the same as (are isomorphic to) the Kripke models from Example~\ref{example-pal-ml} and Example~\ref{example-pal}.
In Example~\ref{example-pal-ml} we showed that $\kPModelP{\kStateS}$ is the result of executing an action model on $\kPModel{\kStateS}$, and in Example~\ref{example-pal} we showed that $\kPModel{\kStateS}$ is the result of executing an action model on $\kPModelP{\kStateS}$.

We note that $\kPModel{\kStateS} \simulatesA \kPModelP{\kStateS}$ via the $\agentA$-refinement $\refinement = \{(\kStateS, \kStateS), (\kStateT, \kStateT)\}$.
We also note that $\kPModelP{\kStateS} \simulatesA \kPModel{\kStateS}$ via the $\agentA$-refinement $\refinement = \{(\kStateS, \kStateS), (\kStateT, \kStateT), (\kStateS, \kStateU)\}$.

This demonstrates that in some situations two non-bisimilar Kripke models can be mutual refinements of one another.
This mirrors Example~\ref{example-pal}, where we demonstrated that in some situations the effects of an action model can be reversed by another action model. 
\end{example}

\pagebreak

As mentioned previously, the existence of a refinement between two Kripke models can be seen as indicating that one Kripke model is the result of an epistemic update of the other.
In the previous examples we saw examples where executing an action model results in a refinement of the original Kripke model.
For the remainder of this section we will discuss properties of refinements, with a particular focus on the relationship between refinements and epistemic updates.

We begin with some properties that follow directly from the definition of refinements.
We note that the conditions for a refinement are weaker than the conditions for a bisimulation. 

\begin{proposition}\label{bisimulation-refinement}
Let $\agentsB \subseteq \agents$ be a set of agents. Then every bisimulation is a $\agentsB$-refinement.
\end{proposition}

\begin{corollary}\label{bisimilar-refinement}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kPModel{\kStateS}$ and $\kPModelP{\kStateSP}$ be pointed Kripke models.
If $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$ then $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
\end{corollary}

We note that in the case where {\bf forth} is required for every agent, the conditions for a refinement are the same as the conditions for a bisimulation.

\begin{proposition}\label{refinement-bisimulation}
Every $\emptyset$-refinement is a bisimulation.
\end{proposition}

\begin{corollary}\label{refinement-bisimilar}
Let $\kPModel{\kStateS}$ and $\kPModelP{\kStateSP}$ be pointed Kripke models.
If $\kPModel{\kStateS} \simulates[\emptyset] \kPModelP{\kStateSP}$ then $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$.
\end{corollary}

We also note that the conditions for a refinement over a given set of agents are weaker than the conditions for a refinement over a subset of the set of agents.

\begin{proposition}
Let $\agentsB \subseteq \agentsC \subseteq \agents$ be sets of agents.
Then every $\agentsB$-refinement is a $\agentsC$-refinement.
\end{proposition}

\begin{corollary}
Let $\agentsB \subseteq \agentsC \subseteq \agents$ be sets of agents and let $\kPModel{\kStateS}$ and $\kPModelP{\kStateSP}$ be pointed Kripke models.
If $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$ then $\kPModel{\kStateS} \simulatesCs \kPModelP{\kStateSP}$.
\end{corollary}

Similar to bisimulations, refinements over sets of agents may be composed to form new refinements.
However as refinements over different sets of agents relax {\bf forth} for different sets of agents, then composing such refinements results in a refinement that relaxes {\bf forth} for both sets of agents.

\begin{proposition}\label{refinement-composition}
Let $\agentsB, \agentsC \subseteq \agents$, let $\kModelAndTuple$, $\kModelAndTupleP$, and $\kModelAndTuplePP$ be Kripke models, let $\refinement \subseteq \kStates \times \kStatesP$ be a $\agentsB$-refinement from $\kModel$ to $\kModelP$, and let $\refinement' \subseteq \kStatesP \times \kStatesPP$ be a $\agentsC$-refinement from $\kModelP$ to $\kModelPP$.
Then $\refinement \circ \refinement' \subseteq \kStates \times \kStatesPP$ is a $(\agentsB \cup \agentsC)$-refinement from $\kModel$ to $\kModelPP$.
\end{proposition}

\begin{proof}
    We will show that the relation $\refinement'' = \refinement \circ \refinement' \subseteq \kStates \times \kStatesPP$ is a $(\agentsB \cup \agentsC)$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{\kStateSPP}$.
We note that $(\kStateS, \kStateSPP) \in \refinement''$ if and only if there exists $\kStateSP \in \kStatesP$ such that $(\kStateS, \kStateSP) \in \refinement$ and $(\kStateSP, \kStateSPP) \in \refinement'$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, $\agentC \in \agents \setminus (\agentsB \cup \agentsC)$, and $(\kStateS, \kStateSPP) \in \refinement''$ where there exists $\kStateSP \in \kStatesP$ such that $(\kStateS, \kStateSP) \in \refinement$ and $(\kStateSP, \kStateSPP) \in \refinement'$.
We show that the conditions {\bf atoms-$\atomP$}, {\bf forth-$\agentC$} and {\bf back-$\agentA$} hold.

\paragraph{atoms-$\atomP$}
As $(\kStateS, \kStateSP) \in \refinement$ from {\bf atoms-$\atomP$} for $\refinement$ we have that $\kStateS \in \kValuation(\atomP)$ if and only if $\kStateSP \in \kValuationP(\atomP)$.
As $(\kStateSP, \kStateSPP) \in \refinement'$ from {\bf atoms-$\atomP$} for $\refinement'$ we have that $\kStateSP \in \kValuationP(\atomP)$ if and only if $\kStateSPP \in \kValuationPP(\atomP)$.
Therefore $\kStateS \in \kValuation(\atomP)$ if and only if $\kStateSPP \in \kValuationPP(\atomP)$.

\paragraph{forth-$\agentC$}
Let $\kStateT \in \kSuccessorsC{\kStateS}$.
As $\agentC \in \agents \setminus (\agentsB \cup \agentsC)$ then $\agentC \in \agents \setminus \agentsB$ and $\agentC \in \agents \setminus \agentsC$, so {\bf forth-$\agentC$} holds for $\refinement$ and $\refinement'$.
As $(\kStateS, \kStateSP) \in \refinement$ from {\bf forth-$\agentC$} for $\refinement$ there exists $\kStateTP \in \kSuccessorsPC{\kStateSP}$ such that $(\kStateT, \kStateTP) \in \refinement$.
As $(\kStateSP, \kStateSPP) \in \refinement'$ from {\bf forth-$\agentC$} for $\refinement'$ there exists $\kStateTPP \in \kSuccessorsPPC{\kStateSPP}$ such that $(\kStateTP, \kStateTPP) \in \refinement'$.
Therefore there exists $\kStateTPP \in \kSuccessorsPPC{\kStateSPP}$ such that $(\kStateT, \kStateTPP) \in \refinement''$.

\paragraph{back-$\agentA$}
Let $\kStateTPP \in \kSuccessorsPPA{\kStateSPP}$.
As $(\kStateSP, \kStateSPP) \in \refinement'$ from {\bf back-$\agentA$} for $\refinement'$ there exists $\kStateTP \in \kSuccessorsPA{\kStateSP}$ such that $(\kStateTP, \kStateTPP) \in \refinement'$.
As $(\kStateS, \kStateSP) \in \refinement$ from {\bf back-$\agentA$} for $\refinement$ there exists $\kStateT \in \kSuccessorsA{\kStateS}$ such that $(\kStateT, \kStateTP) \in \refinement$.
Therefore there exists $\kStateT \in \kSuccessorsA{\kStateS}$ such that $(\kStateT, \kStateTPP) \in \refinement''$.

Therefore $\refinement''$ is a $(\agentsB \cup \agentsC)$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{\kStateSPP}$ and $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$.
\end{proof}

Similar to the relational operator $\bisimilar$ for bisimilarity, we note that the relational operator $\simulatesBs$ for refinements is reflexive and transitive.

\begin{proposition}\label{refinements-preorder}
The relational operator $\simulatesBs$ is a preorder (reflexive and transitive) on Kripke models.
\end{proposition}

\begin{proof}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kPModel{\kStateS}$ be a pointed Kripke model.
By Proposition~\ref{bisimulation-equivalence-relation} we have $\kPModel{\kStateS} \bisimilar \kPModel{\kStateS}$ and by Corollary~\ref{bisimilar-refinement} we have $\kPModel{\kStateS} \simulatesBs \kPModel{\kStateS}$.
Therefore the relation $\simulatesBs$ is reflexive.

Let $\agentsB \subseteq \agents$ be a set of agents and let $\kPModel{\kStateS}$, $\kPModelP{\kStateSP}$ and $\kPModelPP{\kStateSPP}$ be pointed Kripke models such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$ and $\kPModelP{\kStateSP} \simulatesBs \kPModelPP{\kStateSPP}$.
Then there exists $\agentsB$-refinements $\refinement \subseteq \kStates \times \kStatesP$ from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSP}$ and $\refinement' \subseteq \kStatesP \times \kStatesPP$ from $\kPModelP{\kStateSP}$ to $\kPModelPP{\kStateSPP}$.
By Proposition~\ref{refinement-composition} the relation $\refinement'' = \refinement \circ \refinement' \subseteq \kStates \times \kStatesPP$ is a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{\kStateSPP}$ and therefore $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSPP}$.
\end{proof}

However as refinements require {\bf back} for every agent, but do not generally require {\bf forth} for every agent, we note that $\simulatesBs$ is not symmetrical when $\agentsB$ is non-empty.
This was demonstrated in Example~\ref{example-refinement-pal}, where Kripke models are given such that  $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateS}$ but $\kPModelP{\kStateS} \not\simulatesBs \kPModel{\kStateS}$.

The refinements of a Kripke model may be characterised as being formed by taking a bisimilar Kripke model, such as by duplicating states and their relationships (satisfying {\bf atoms}, {\bf forth}, and {\bf back}) and then removing edges for some agents (relaxing {\bf forth} for some agents).
We formalise this intuition using the notion of an expanded refinement.
An expanded refinement is a refinement from one Kripke model to another Kripke model where every state in the latter is mapped by the refinement at most one state from the former.

\pagebreak

\begin{definition}\label{expanded-refinement}
Let $\agentsB \subseteq \agents$ be a set of agents, let $\kModelAndTuple$ and $\kModelAndTupleP$, and let $\refinement \subseteq \kStates \times \kStatesP$ be a $\agentsB$-refinement from $\kModel$ to $\kModelP$.
Then $\refinement$ is an {\em expanded $\agentsB$-refinement from $\kModel$ to $\kModelP$} if and only if for every $\kStateSP \in \kStatesP$ there is a unique $\kStateS \in \kStates$ such that $(\kStateS, \kStateSP) \in \refinement$.
\end{definition}

Every refinement is bisimilar to a Kripke model with an expanded refinement.

\begin{lemma}\label{refinement-expansion}
Let $\agentsB \subseteq \agents$ be a set of agents, let $\kPModel{\kStateS}$ and $\kPModelP{\kStateSP}$ be pointed Kripke models such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$, and let $\refinement \subseteq \kStates \times \kStatesP$ be a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSP}$.
Then there exists a pointed Kripke model $\kPModelPP{\kStateSPP}$ such that $\kPModelP{\kStateSP} \bisimilar \kPModelPP{\kStateSPP}$ and $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSPP}$ via an expanded $\agentsB$-refinement.
\end{lemma}

\begin{proof}
We define $\kPModelAndTuplePP{(\kStateS, \kStateSP)}$ where:
\begin{eqnarray*}
    \kStatesPP &=& \refinement\\
    \kAccessibilityPPA &=& \{\big((\kStateT, \kStateTP), (\kStateU, \kStateUP)\big) \in \kStatesPP \times \kStatesPP \mid (\kStateT, \kStateU) \in \kAccessibilityA, (\kStateTP, \kStateUP) \in \kAccessibilityPA\}\\
    \kValuationPP(\atomP) &=& \{(\kStateT, \kStateTP) \in \kStatesPP \mid \kStateTP \in \kValuationP(\atomP)\}
\end{eqnarray*}
We will show that $\kPModel{\kStateS} \simulatesBs \kPModelPP{(\kStateS, \kStateSP)}$ and $\kPModelP{\kStateSP} \bisimilar \kPModelPP{(\kStateS, \kStateSP)}$.

Let $\refinement' \subseteq \kStates \times \kStatesPP$ be a relation where:
$$
\refinement' = \{(\kStateT, (\kStateT, \kStateTP)) \mid (\kStateT, \kStateTP) \in \refinement\}
$$
By construction every $(\kStateT, \kStateTP) \in \kStateSPP$ has the unique $\kStateT \in \kStates$ such that $(\kStateT, (\kStateT, \kStateTP)) \in \refinement'$.
We show that $\refinement'$ is a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{(\kStateS, \kStateSP)}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, $\agentC \in \agents \setminus \agentsB$, and $(\kStateT, (\kStateT, \kStateTP)) \in \refinement'$.

\paragraph{atoms-$\atomP$}
By {\bf atoms-$\atomP$} for $\refinement$, $\kStateT \in \kValuation(\atomP)$ if and only if $\kStateTP \in \kValuationP(\atomP)$.
By construction $\kStateTP \in \kValuationP(\atomP)$ if and only if $(\kStateT, \kStateTP) \in \kValuationPP(\atomP)$.

\paragraph{forth-$\agentC$}
Let $\kStateU \in \kSuccessorsC{\kStateT}$.
By {\bf forth-$\agentC$} for $\refinement$ there exists $\kStateUP \in \kSuccessorsPC{\kStateTP}$ such that $(\kStateU, \kStateUP) \in \refinement$.
By construction $(\kStateU, \kStateUP) \in \kSuccessorsPPC{(\kStateT, \kStateTP)}$ and $(\kStateU, (\kStateU, \kStateUP)) \in \refinement'$.

\paragraph{back-$\agentA$}
Let $(\kStateU, \kStateUP) \in \kSuccessorsPPA{(\kStateT, \kStateTP)}$.
By construction $\kStateU \in \kSuccessorsA{\kStateT}$ and $(\kStateU, (\kStateU, \kStateUP)) \in \refinement'$.

Therefore $\refinement'$ is a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{(\kStateS, \kStateSP)}$ and $\kPModel{\kStateS} \simulatesBs \kPModelPP{(\kStateS, \kStateSP)}$ via an expanded $\agentsB$-refinement.

Let $\bisimulation'' \subseteq \kStatesP \times \kStatesPP$ be a relation where:
$$
\bisimulation'' = \{(\kStateTP, (\kStateT, \kStateTP)) \mid (\kStateT, \kStateTP) \in \refinement\}
$$
We show that $\bisimulation''$ is a bisimulation between $\kPModelP{\kStateSP}$ and $\kPModelPP{(\kStateS, \kStateSP)}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, and $(\kStateTP, (\kStateT, \kStateTP)) \in \bisimulation''$.

\paragraph{atoms-$\atomP$}
By construction $\kStateTP \in \kValuationP(\atomP)$ if and only if $(\kStateT, \kStateTP) \in \kValuationPP(\atomP)$.

\paragraph{forth-$\agentA$}
Let $\kStateUP \in \kSuccessorsPA{\kStateTP}$.
By {\bf back-$\agentA$} for $\refinement$ there exists $\kStateU \in \kSuccessorsA{\kStateT}$ such that $(\kStateU, \kStateUP) \in \refinement$.
By construction $(\kStateU, \kStateUP) \in \kSuccessorsPPA{(\kStateT, \kStateTP)}$ and $(\kStateU, (\kStateUP, \kStateUP)) \in \bisimulation''$.

\paragraph{back-$\agentA$}
Let $(\kStateU, \kStateUP) \in \kSuccessorsPPA{(\kStateT, \kStateTP)}$.
By construction $\kStateUP \in \kSuccessorsPA{\kStateTP}$ and $(\kStateUP, (\kStateU, \kStateUP)) \in \bisimulation''$.

Therefore $\bisimulation''$ is a bisimulation between $\kPModelP{\kStateSP}$ and $\kPModelPP{(\kStateS, \kStateSP)}$ and $\kPModelP{\kStateSP} \bisimilar \kPModelPP{(\kStateS, \kStateSP)}$.
\end{proof}

To formalise our intuition about refinements, we show a more general result.
Using the notion of an expanded refinement we show that we can decompose a refinement over a set of agents into refinements over smaller sets of agents, giving us the converse of Proposition~\ref{refinement-composition}.

\begin{proposition}\label{refinement-decomposition}
Let $\agentsB, \agentsC \subseteq \agents$ be sets of agents, and let $\kPModelAndTuple{\kStateS}$ and $\kPModelAndTuplePP{\kStateSPP}$ be pointed Kripke models such that $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$.
Then there exists a pointed Kripke model $\kPModelP{\kStateSP}$ such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPP{\kStateSPP}$.
\end{proposition}

\begin{proof}
By Lemma~\ref{refinement-expansion} there exists a pointed Kripke model $\kPModelPPP{\kStateSPPP}$ such that $\kPModelPP{\kStateSPP} \bisimilar \kPModelPPP{\kStateSPPP}$ and $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPPP{\kStateSPPP}$ via an expanded $(\agentsB \cup \agentsC)$-refinement.
Suppose that there exists a pointed Kripke model $\kPModelP{\kStateSP}$ such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPPP{\kStateSPPP}$.
As $\kPModelPP{\kStateSPP} \bisimilar \kPModelPPP{\kStateSPPP}$ then by Corollary~\ref{bisimilar-refinement} we have that $\kPModelPPP{\kStateSPPP} \simulatesCs \kPModelPP{\kStateSPP}$ and by Proposition~\ref{refinements-preorder} we have that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPP{\kStateSPP}$.

Then without loss of generality we assume that $\kPModelAndTuplePP{\kStateSPP}$ is such that $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$ via an expanded $(\agentsB \cup \agentsC)$-refinement $\refinement \subseteq \kStates \times \kStatesPP$.

We define $\kPModelAndTupleP{\kStateSPP}$ where:
\begin{eqnarray*}
    \kStatesP &=& \kStatesPP\\
    \kAccessibilityPB &=& \kAccessibilityPPB\\
    \kAccessibilityPC &=& \{(\kStateTPP, \kStateUPP) \in \kStatesP \times \kStatesP \mid (\refinement^{-1}(\kStateTPP), \refinement^{-1}(\kStateUPP)) \in \kAccessibilityC \}\\
    \kValuationP &=& \kValuationPP
\end{eqnarray*}
where $\agentB \in \agents \setminus \agentsC$, $\agentC \in \agentsC$, and for every $\kStateTPP \in \kStatesPP$ we denote by $\refinement^{-1}(\kStateTPP)$ the unique $\kStateT \in \kStates$ such that $(\kStateT, \kStateTPP) \in \refinement^{-1}$.

We note for every $\agentC \in \agentsC$ that $\kAccessibilityPPC \subseteq \kAccessibilityPC$ as for every $(\kStateTPP, \kStateUPP) \in \kAccessibilityPPC$ by {\bf back-$\agentC$} for $\refinement$ we have that $(\refinement^{-1}(\kStateTPP), \refinement^{-1}(\kStateUPP)) \in \kAccessibilityC$ and therefore $(\kStateTPP, \kStateUPP) \in \kAccessibilityPPC$ by the definition of $\kAccessibilityPPC$.

We will show that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSPP} \simulatesCs \kPModelPP{\kStateSPP}$.

We show that $\refinement$ is a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSPP}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, $\agentD \in \agents \setminus \agentsB$, and $(\kStateT, \kStateTPP) \in \refinement$.

\paragraph{atoms-$\atomP$}
By {\bf atoms-$\atomP$} for $\refinement$, $\kStateT \in \kValuation(\atomP)$ if and only if $\kStateTPP \in \kValuationPP(\atomP)$.
By construction $\kStateTPP \in \kValuationPP(\atomP)$ if and only if $\kStateTPP \in \kValuationPP(\atomP)$.

\paragraph{forth-$\agentD$}
Let $\kStateU \in \kSuccessorsD{\kStateT}$.
By {\bf forth-$\agentD$} for $\refinement$ there exists $\kStateUPP \in \kSuccessorsPPD{\kStateTPP}$ such that $(\kStateU, \kStateUPP) \in \refinement$.
If $\agentD \in \agentsC$ then from above we have that $\kAccessibilityPPD \subseteq \kAccessibilityPD$, and if $\agentD \notin \agentsC$ then $\kAccessibilityPD = \kAccessibilityPPD$.
Then $\kSuccessorsPPD{\kStateTPP} \subseteq \kSuccessorsPD{\kStateTPP}$ so $\kStateUPP \in \kSuccessorsPD{\kStateTPP}$.

\paragraph{back-$\agentA$}
Let $\kStateUPP \in \kSuccessorsPA{\kStateTPP}$.
By construction we must have $\refinement^{-1}(\kStateUPP) \in \kSuccessorsA{\refinement^{-1}(\kStateTPP)}$.
As $(\kStateT, \kStateTPP) \in \refinement$ then $\refinement^{-1}(\kStateTPP) = \kStateT$ so $\refinement^{-1}(\kStateUPP) \in \kSuccessorsA{\refinement^{-1}(\kStateTPP)}$ and $(\refinement^{-1}(\kStateUPP), \kStateUPP) \in \refinement$.

Therefore $\refinement$ is a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSPP}$ and $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSPP}$.

We define $\refinement' \subseteq \kStatesP \times \kStatesPP$ where:
$$
\refinement' = \{(\kStateTPP, \kStateTPP) \mid \kStateTPP \in \kStatesPP\}
$$

We show that $\refinement'$ is a $\agentsC$-refinement from $\kPModelP{\kStateSPP}$ to $\kPModelPP{\kStateSPP}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, $\agentD \in \agents \setminus \agentsC$, and $(\kStateTPP, \kStateTPP) \in \refinement$.

\paragraph{atoms-$\atomP$}
By construction $\kStateTPP \in \kValuationP(\atomP)$ if and only if $\kStateTPP \in \kValuationPP(\atomP)$.

\paragraph{forth-$\agentD$}
Let $\kStateUP \in \kSuccessorsPD{\kStateTPP}$.
As $\agentD \notin \agentsC$ then by construction $\kStateUP \in \kSuccessorsPPD{\kStateTPP}$ and $(\kStateUP, \kStateUP) \in \refinement'$.

\paragraph{back-$\agentA$}
Let $\kStateUPP \in \kSuccessorsPPA{\kStateTPP}$.
By construction $\kSuccessorsPPA{\kStateTPP} \subseteq \kSuccessorsPA{\kStateTPP}$ so $\kStateUPP \in \kSuccessorsPA{\kStateTPP}$ and $(\kStateUPP, \kStateUPP) \in \refinement$.

Therefore $\refinement'$ is a $\agentsC$-refinement from $\kPModelP{\kStateSPP}$ to $\kPModelPP{\kStateSPP}$ and $\kPModelP{\kStateSPP} \simulatesCs \kPModelPP{\kStateSPP}$.

Therefore $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSPP} \simulatesCs \kPModelPP{\kStateSPP}$.
\end{proof}

Given this result we capture our intuition about refinements with a corollary.

\begin{corollary}\label{refinement-subrelations}
Let $\agentsB \subseteq \agents$ be a set of agents, and let $\kPModelAndTuple{\kStateS}$ and $\kPModelAndTuplePP{\kStateSPP}$ be pointed Kripke models such that $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSPP}$.
Then there exists a pointed Kripke model $\kPModelP{\kStateSPP} = ((\kStatesPP, \kAccessibilityP{}, \kValuationPP), \kStateSPP)$ such that $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$ and for every $\agentA \in \agents$ if $\agentA \in \agentsB$ then $\kAccessibilityPPA \subseteq \kAccessibilityPA$, and if $\agentA \notin \agentsB$ then $\kAccessibilityPPA = \kAccessibilityPA$.
\end{corollary}

\begin{proof}
By Proposition~\ref{refinement-decomposition} there exists a pointed Kripke model $\kPModelAndTupleP{\kStateSP}$ such that $\kPModel{\kStateS} \simulates[\emptyset] \kPModelP{\kStateSP} \simulatesBs \kPModelPP{\kStateSPP}$.
As in the proof of Proposition~\ref{refinement-decomposition}, $\kPModel{\kStateS} \simulates[\emptyset] \kPModelP{\kStateSP}$ via an expanded $\emptyset$-refinement $\refinement \subseteq \kStates \times \kStatesP$.
As $\kPModel{\kStateS} \simulates[\emptyset] \kPModelP{\kStateSP}$ then by Corollary~\ref{refinement-bisimilar} we have that $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$.
We note that using the construction of Proposition~\ref{refinement-decomposition}, we have a model $\kPModelP{\kStateSP}$ such that for every $\agentA \in \agents$ if $\agentA \in \agentsB$ then $\kAccessibilityPPA \subseteq \kAccessibilityPA$, and if $\agentA \notin \agentsB$ then $\kAccessibilityPPA = \kAccessibilityPA$.
\end{proof}

Here we see that every refinement of a Kripke model may be formed by taking a bisimilar Kripke model and removing edges for some agents.
As a kind of converse to this result we show that every Kripke model formed by taking a bisimilar Kripke model and removing edges for some agents is a refinement.

\begin{proposition}\label{subrelations-refinement}
Let $\agentsB \subseteq \agents$ be a set of agents, let $\kPModelAndTuple{\kStateS}$, $\kPModelAndTupleP{\kStateSP}$, and $\kPModelPP{\kStateSP} = ((\kStatesP, \kAccessibilityPP{}, \kValuationP), \kStateSP)$ be pointed Kripke models such that $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$ and for every $\agentA \in \agents$ if $\agentA \in \agentsB$ then $\kAccessibilityPPA \subseteq \kAccessibilityPA$ and if $\agentA \notin \agentsB$ then $\kAccessibilityPPA = \kAccessibilityPA$.
Then $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSP}$.
\end{proposition}

\begin{proof}
As $\kPModel{\kStateS} \bisimilar \kPModelP{\kStateSP}$ by Proposition~\ref{bisimilar-refinement} we have that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
So we need only show that $\kPModelP{\kStateSP} \simulatesBs \kPModelPP{\kStateSP}$ and $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSP}$ will follow by the transitivity of $\simulatesBs$ shown in Proposition~\ref{refinements-preorder}.

Let $\refinement \subseteq \kStatesP \times \kStatesPP$ where $\refinement = \{(\kStateTP, \kStateTP) \mid \kStateTP \in \kStatesP\}$.
We show that $\refinement$ is a $\agentsB$-refinement from $\kPModelP{\kStateSP}$ to $\kPModelPP{\kStateSPP}$.
Let $(\kStateTP, \kStateTP) \in \refinement$ where $\kStateTP \in \kStatesP$, and let $\atomP \in \atoms$, $\agentC \in \agents \setminus \agentsB$, and $\agentA \in \agents$.

\paragraph{atoms-$\atomP$}
Trivial as $\kModelP$ and $\kModelPP$ have the same valuation.

\paragraph{forth-$\agentC$}
Let $\kStateUP \in \kSuccessorsPC{\kStateTP}$.
As $\agentC \notin \agentsB$ then by construction $\kAccessibilityPPA = \kAccessibilityPA$, so $\kStateUP \in \kSuccessorsPPC{\kStateTP}$.
By construction $(\kStateUP, \kStateUP) \in \refinement$.

\paragraph{back-$\agentA$}
Let $\kStateUP \in \kSuccessorsPPA{\kStateTP}$.
By construction if $\agentA \in \agentsB$ then $\kAccessibilityPPA \subseteq \kAccessibilityPA$, and if $\agentA \notin \agentsB$ then $\kAccessibilityPPA = \kAccessibilityPA$.
Either way $\kAccessibilityPPA \subseteq \kAccessibilityPA$, so $\kStateUP \in \kSuccessorsPA{\kStateTP}$.
By construction $(\kStateUP, \kStateUP) \in \refinement$.

Therefore $\refinement$ is a $\agentsB$-refinement from $\kPModelP{\kStateSP}$ to $\kPModelPP{\kStateSPP}$ and $\kPModelP{\kStateSP} \simulatesBs \kPModelPP{\kStateSP}$.

Therefore $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSP}$.
\end{proof}

We note that our definition of a refinement is more general than previous definitions.
van Ditmarsch and French~\cite{vanditmarsch:2009} gave a definition corresponding to our notion of an $\agents$-refinement, not requiring {\bf forth} at all.
van Ditmarsch, French and Pinchinat~\cite{vanditmarsch:2010} subsequently gave a definition corresponding to our notion of a $\agentA$-refinement, relaxing {\bf forth} for a single agent.
However we may alternatively define our notion of a $\agentsB$-refinement as the composition of $\agentA$-refinements, through intermediate Kripke models.

\begin{proposition}\label{refinement-agent-decomposition}
Let $\agentsB, \agentsC \subseteq \agents$ be sets of agents, and let $\kPModel{\kStateS}$ and $\kPModelPP{\kStateSPP}$ be pointed Kripke models.
Then $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$ if and only if there exists a pointed Kripke model $\kPModelP{\kStateSP}$ such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPP{\kStateSPP}$.
\end{proposition}

\begin{proof}
Suppose that $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$.
Then by Proposition~\ref{refinement-decomposition} there exists a pointed Kripke model $\kPModelP{\kStateSP}$ such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPP{\kStateSPP}$.

Suppose that there exists a pointed Kripke model $\kPModelP{\kStateSP}$ such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP} \simulatesCs \kPModelPP{\kStateSPP}$, via a $\agentsB$-refinement $\refinement$ from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSP}$ and a $\agentsC$-refinement $\refinement'$ from $\kPModelP{\kStateSP}$ to $\kPModelPP{\kStateSPP}$.
Then by Proposition~\ref{refinement-composition} the relation $\refinement \circ \refinement'$ is a $(\agentsB \cup \agentsC)$-refinement from $\kPModel{\kStateS}$ to $\kPModelPP{\kStateSPP}$ and so $\kPModel{\kStateS} \simulates[(\agentsB \cup \agentsC)] \kPModelPP{\kStateSPP}$.
\end{proof}

Given this we can see a correspondence between our notion of $\agentsB$-refinements and the more restricted notion of $\agentA$-refinements used by van Ditmarsch, French and Pinchinat~\cite{vanditmarsch:2010}.
Specifically we note that given a set of two or more agents $\agentsB = \{\agentB_1, \agentB_2, \dots, \agentB_n\}$ we can express the fact that $\kPModel{\kStateS} \simulatesBs \kPModelPP{\kStateSPP}$ by saying that there exists a series of intermediate refinements such that $\kPModel{\kStateS} \simulates[\agentB_1] \cdots \simulates[\agentB_n] \kPModelP{\kStateSP}$.

\begin{corollary}
Let $\agentsB = \{\agentB_1, \agentB_2, \dots, \agentB_n\}$ be a set of two or more agents and let $\kPModel{\kStateS}$ and $\kPModelP{\kStateS}$ be pointed Kripke models.
Then $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$ if and only if there exists pointed Kripke models $\kPModel[1]{\kStateS[1]}, \kPModel[2]{\kStateS[2]}, \dots, \kPModel[n - 1]{\kStateS[n - 1]}$ such that $\kPModel{\kStateS} \simulates[\agentB_1] \kPModel[1]{\kStateS[1]} \simulates[\agentB_2] \kPModel[2]{\kStateS[2]} \simulates[\agentB_3] \cdots \simulates[\agentB_{n - 1}] \kPModel[n - 1]{\kStateS[n - 1]} \simulates[\agentB_n] \kPModelP{\kStateSP}$.
\end{corollary}

\begin{proof}
We can show by induction over $i = 2, 3, \dots, n$ that the claim holds for $\agentsB_i = \{\agentB_1, \agentB_2, \dots, \agentB_i\}$, 
in the base case using Proposition~\ref{refinement-agent-decomposition} directly where $\agentsB_2 = \{\agentB_1, \agentB_2\}$, and
in the inductive case using Proposition~\ref{refinement-agent-decomposition} again, where $\agentsB_{i + 1} = \agentsB_i \cup \{\agentB_{i + 1}\}$.
\end{proof}

van Ditmarsch and French~\cite{vanditmarsch:2009} motivated their work in refinement modal logic by observing that refinements correspond to a very general notion of epistemic updates, in accordance with our informal understanding of epistemic updates as purely informative and monotonically increasing certainty about information.
We now attempt to formalise this general notion of epistemic updates.

Following the general model used by public announcements and action models, we model an epistemic update as a transition from one pointed Kripke model, $\kPModel{\kStateS}$ to another, $\kPModelP{\kStateSP}$.
When we describe an epistemic update as purely informative we mean that this transition preserves the truth of propositional atoms and their negations.
If $\kPModel{\kStateS} \entails \atomP$ then we require that $\kPModelP{\kStateSP} \entails \atomP$ and if $\kPModel{\kStateS} \entails \lnot \atomP$ then we require that $\kPModelP{\kStateSP} \entails \lnot \atomP$.
However it's less clear what we mean when we say that epistemic updates increase certainty of information monotonically.
Intuitively we mean that an epistemic update cannot cause an agent to forget or revise information that they were previously certain of, but this leaves open the question of what information agents are ``certain of''.
At a first approximation we might say that epistemic updates should preserve all knowledge.
That is, anything that an agent knows before an epistemic update, the agent should continue to know after an epistemic update.
When we only consider knowledge of the truth of propositional atoms then this approximation seems reasonable.
Epistemic updates preserve the truth of propositional atoms, so if an agent knows that a propositional atom is true, then the agent can be certain that this information won't change as the result of an epistemic update, so after an epistemic update the agent should continue to know that the propositional atom is true.
If $\kPModel{\kStateS} \entails \knowsA \atomP$ then we require that $\kPModelP{\kStateSP} \entails \knowsA \atomP$.
However the truth of all information should not always be preserved by epistemic updates.
We expect that epistemic updates should provide agents with additional information, so there should be situations where an agent knows something after an epistemic update that they didn't know before the epistemic update.
For example, if an agent doesn't know that a propositional atom is true before an epistemic update, then it's reasonable for the agent to know that the propositional atom is true after an epistemic update.
If $\kPModel{\kStateS} \entails \lnot \knowsA \atomP$ this shouldn't rule out $\kPModelP{\kStateSP} \entails \knowsA \atomP$.
So information about a lack of knowledge should not always be preserved by epistemic updates.
Then if an agent has information that is true before an epistemic update, but the truth of the information isn't preserved by an epistemic updates, then it's reasonable, and in some cases expected, that after the epistemic update the agent no longer knows that the information is true.
If $\kPModel{\kStateS} \entails \knowsA \lnot \knowsB \atomP$ and $\kPModelP{\kStateSP} \entails \knowsB \atomP$ this shouldn't prevent $\kPModelP{\kStateSP} \entails \lnot \knowsA \lnot \knowsB \atomP$.
So rather than our first approximation, that epistemic updates should preserve all knowledge, we see that it's more reasonable that epistemic updates only preserve knowledge about information that has its truth preserved by epistemic updates.
We formalise this notion with the definition of positive formulas.

\begin{definition}[Positive formulas]
Let $\agentsB \subseteq \agents$ be a set of agents.
The {\em language of $\agentsB$-positive formulas}, \langMlPlusBs{}, is inductively defined as:
$$
\phi ::= 
    \atomP \mid
    \lnot \atomP \mid
    (\phi \land \phi) \mid
    (\phi \lor \phi) \mid
    \necessaryA \phi \mid
    \possibleC \phi
$$
where $\atomP \in \atoms$, $\agentA \in \agents$ and $\agentC \in \agents \setminus \agentsB$.
\end{definition}

We call an $\agents$-positive formula simply a {\em positive formula} and we call an $\{\agentA\}$-positive formula simply an {\em $\agentA$-positive formula}.

Restricting our attention for a moment to only the $\agents$-positive formulas, where all $\possibleA$~modalities are prohibited, we note that this captures syntactically our intuition of which statements should have their truth preserved by epistemic updates in general.
As a base case, propositional atoms and their negations should have their truth preserved by epistemic updates.
Then given two statements that have their truth preserved by epistemic updates, a conjunction or disjunction of the two statements should have its truth preserved by epistemic updates.
Finally, given a statement that has its truth preserved by epistemic updates, knowledge of that statement should be preserved by epistemic updates. 

Considering the more general case of $\agentsB$-positive formulas, where $\possibleA$~modalities are prohibited only for agents in the set $\agentsB$, we note that this captures syntactically an intuition about which statements should have their truth preserved by epistemic updates, when only the agents in $\agentsB$ may be provided with additional information.
Roughly, anything an agent {\em not} in $\agentsB$ {\em doesn't} know before an epistemic update, the agent should continue not knowing after an epistemic update.
For example, supposing that $\agentA \notin \agentsB$ then if $\kPModel{\kStateS} \entails \lnot \knowsA \atomP$, this is equivalent to $\kPModel{\kStateS} \entails \possibleA \lnot \atomP$, so we require that $\kPModelP{\kStateSP} \entails \possibleA \lnot \atomP$, or equivalently that $\kPModelP{\kStateSP} \entails \lnot \knowsA \atomP$.

Returning to refinements, we now consider the relationship between refinements and positive formulas.
van Ditmarsch and French~\cite{vanditmarsch:2009} showed that $\agents$-refinements preserve the truth of $\agents$-positive formulas.
We generalise this result to our more general notion of refinements, showing that $\agentsB$-refinements preserve the truth of $\agentsB$-positive formulas.

\begin{proposition}\label{refinements-preserve-positive}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kPModel{\kStateS}$ and $\kPModelP{\kStateSP}$ be pointed Kripke models such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
For every $\phi \in \langMlPlusBs$
if $\kPModel{\kStateS} \entails \phi$ then $\kPModelP{\kStateSP} \entails \phi$.
\end{proposition}

\begin{proof}
Let $\phi \in \langMlPlusBs$.
As $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$ there exists a $\agentsB$-refinement $\refinement \subseteq \kStates \times \kStatesP$ such that $(\kStateS, \kStateSP) \in \refinement$.
We show for every $(\kStateT, \kStateTP) \in \refinement$ that $\kPModel{\kStateT} \entails \phi$ implies $\kPModelP{\kStateTP} \entails \phi$ by induction on the structure of $\phi$.
Let $(\kStateT, \kStateTP) \in \refinement$.

Suppose that $\phi = \atomP$ where $\atomP \in \atoms$ and suppose that $\kPModel{\kStateT} \entails \atomP$.
As $(\kStateT, \kStateTP) \in \refinement$ then by {\bf atoms-$\atomP$} we have that $\kPModelP{\kStateTP} \entails \atomP$.

Suppose that $\phi = \lnot \atomP$ where $\atomP \in \atoms$ and suppose that $\kPModel{\kStateT} \entails \lnot \atomP$.
As $(\kStateT, \kStateTP) \in \refinement$ then by {\bf atoms-$\atomP$} we have that $\kPModelP{\kStateTP} \entails \lnot \atomP$.

Suppose that $\phi = \psi \land \chi$ or $\phi = \psi \lor \chi$ where $\psi, \chi \in \langMlPlusBs$.
These follow directly from the induction hypothesis.

Suppose that $\phi = \necessaryA \psi$ where $\agentA \in \agents$ and $\psi \in \langMlPlusBs$, and suppose that $\kPModel{\kStateT} \entails \necessaryA \psi$.
Then $\kPModel{\kStateU} \entails \psi$ for every $\kStateU \in \kSuccessorsA{\kStateT}$.
Let $\kStateUP \in \kSuccessorsPA{\kStateTP}$.
As $(\kStateT, \kStateTP) \in \refinement$ then by {\bf back-$\agentA$} there exists $\kStateU \in \kSuccessorsA{\kStateT}$ such that $(\kStateU, \kStateUP) \in \refinement$.
As $(\kStateU, \kStateUP) \in \refinement$ and $\kPModel{\kStateU} \entails \psi$ then by the induction hypothesis we have $\kPModelP{\kStateUP} \entails \psi$.
So for every $\kStateUP \in \kSuccessorsPA{\kStateTP}$ we have $\kPModelP{\kStateUP} \entails \psi$.
Therefore $\kPModelP{\kStateTP} \entails \necessaryA \psi$.

Suppose that $\phi = \possibleC \psi$ where $\agentC \in \agents \setminus \agentsB$ and $\psi \in \langMlPlusBs$, and suppose that $\kPModel{\kStateT} \entails \possibleC \psi$.
Then there exists $\kStateU \in \kSuccessorsC{\kStateT}$ such that $\kPModel{\kStateU} \entails \psi$.
As $(\kStateT, \kStateTP) \in \refinement$ then by {\bf forth-$\agentC$} there exists $\kStateUP \in \kSuccessorsPC{\kStateTP}$ such that $(\kStateU, \kStateUP) \in \refinement$.
As $(\kStateU, \kStateUP) \in \refinement$ and $\kPModel{\kStateU} \entails \psi$ then by the induction hypothesis we have $\kPModelP{\kStateUP} \entails \psi$.
Therefore $\kPModelP{\kStateTP} \entails \possibleC \psi$.

Therefore if $\kPModel{\kStateS} \entails \phi$ then $\kPModelP{\kStateSP} \entails \phi$.
\end{proof}

We compare this result for refinements to the analogous result, Proposition~\ref{modal-bisimulation-invariance} for bisimulations, which says that bisimulations preserve the truth of all modal formulas.
This result is actually a generalisation of the result for bisimulations, as $\emptyset$-refinements are bisimulations and every modal formula is equivalent to an $\emptyset$-positive formula, which is the same as a formula in negation normal form.
Compared to bisimulations, in the general case $\agentsB$-refinements relax the {\bf forth} condition for the agents in $\agentsB$, and this is why the truth of $\possibleB$ operators for $\agentB \in \agentsB$ are not preserved by refinements in general.

Similar to bisimulations we also have the converse in the case of modally saturated Kripke models.

\begin{proposition}\label{refinements-hennessy-milner}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kModel$ and $\kModelP$ be modally saturated Kripke models such that for every $\phi \in \langMlPlusBs$ if $\kPModel{\kStateS} \entails \phi$ then $\kPModelP{\kStateSP} \entails \phi$.
Then $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
\end{proposition}

\begin{proof}
Let $\refinement \subseteq \kStates \times \kStatesP$ be a relation such that $(\kStateT, \kStateTP) \in \refinement$ if and only if for every $\phi \in \langMlPlusBs$ if $\kPModel{\kStateT} \entails \phi$ then $\kPModelP{\kStateTP} \entails \phi$.
We will show that the $\refinement$ is a $\agentsB$-refinement and therefore $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$, $\agentC \in \agents \setminus \agentsB$ and $(\kStateT, \kStateTP) \in \refinement'$.
We show that the conditions {\bf atoms-$\atomP$}, {\bf forth-$\agentC$} and {\bf back-$\agentA$} hold.

\paragraph{atoms-$\atomP$}
$\kStateT \in \kValuation(\atomP)$ if and only if $\kPModel{\kStateT} \entails \atomP$.
As $\atomP \in \langMlPlusBs$ and $(\kStateT, \kStateTP) \in \refinement$ then $\kPModel{\kStateT} \entails \atomP$ if and only if $\kPModelP{\kStateTP} \entails \atomP$
and $\kPModelP{\kStateTP} \entails \atomP$ if and only if $\kStateTP \in \kValuationP(\atomP)$.
Therefore $\kStateT \in \kValuation(\atomP)$ if and only if $\kStateTP \in \kValuationP(\atomP)$.

\paragraph{forth-$\agentC$}
Let $\kStateU \in \kSuccessorsC{\kStateT}$, let $\Sigma = \{\phi \in \langMlPlusBs \mid \kPModel{\kStateU} \entails \phi\}$ be the set of $\agentsB$-positive formulas satisfied at $\kPModel{\kStateU}$, and let $\Delta \subseteq \Sigma$ be a finite subset of $\Sigma$.
Then $\kPModel{\kStateU} \entails \bigwedge \Delta$ and so $\kPModel{\kStateT} \entails \possibleC \bigwedge \Delta$.
As $\possibleC \bigwedge \Delta \in \langMlPlusBs$ and $(\kStateT, \kStateTP) \in \refinement$ then $\kPModelP{\kStateTP} \entails \possibleC \bigwedge \Delta$.
So $\Sigma$ is finitely satisfiable on $\kPModelP{\kSuccessorsPC{\kStateTP}}$ and as $\kModelP$ is modally saturated then $\Sigma$ is satisfiable on $\kPModelP{\kSuccessorsPC{\kStateTP}}$.
So there exists $\kStateUP \in \kSuccessorsPC{\kStateTP}$ such that $\kPModelP{\kStateUP} \entails \Sigma$, and so for every $\phi \in \langMlPlusBs$ if $\kPModel{\kStateU} \entails \phi$ then $\kPModelP{\kStateUP} \entails \phi$.
Therefore $(\kStateU, \kStateUP) \in \refinement$.

\paragraph{back-$\agentA$}
Let $\kStateUP \in \kSuccessorsPA{\kStateTP}$, let $\Sigma = \{\phi \in \langMlPlusBs \mid \kPModelP{\kStateUP} \nentails \phi\}$ be the set of $\agentsB$-positive formulas {\em not} satisfied at $\kPModelP{\kStateUP}$, and let $\Delta \subseteq \Sigma$ be a finite subset of $\Sigma$.
Then $\kPModelP{\kStateUP} \entails \bigwedge \lnot \Delta$ and so $\kPModelP{\kStateTP} \entails \possibleA \bigwedge \lnot \Delta$, or equivalently $\kPModelP{\kStateTP} \nentails \necessaryA \bigvee \Delta$.
As $\necessaryA \bigvee \Delta \in \langMlPlusBs$ and $(\kStateT, \kStateTP) \in \refinement$ then $\kPModel{\kStateT} \nentails \necessaryA \bigvee \Delta$, or equivalently $\kPModel{\kStateT} \entails \possibleA \bigwedge \lnot \Delta$.
So $\lnot \Sigma$ is finitely satisfiable on $\kPModel{\kSuccessorsA{\kStateT}}$ and as $\kModel$ is modally saturated then $\lnot \Sigma$ is satisfiable on $\kPModel{\kSuccessorsA{\kStateT}}$.
So there exists $\kStateU \in \kSuccessorsA{\kStateT}$ such that $\kPModel{\kStateU} \entails \lnot \Sigma$, and so for every $\phi \in \langMlPlusBs$ if $\kPModel{\kStateU} \entails \phi$ then $\kPModelP{\kStateUP} \entails \phi$.
Therefore $(\kStateU, \kStateUP) \in \refinement$.

Therefore $\refinement$ is a $\agentsB$-refinement and $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
\end{proof}

If we consider the preservation of positive formulas to be a minimal requirement for epistemic updates, then taken together Proposition~\ref{refinements-preserve-positive} and Proposition~\ref{refinements-hennessy-milner} form a strong case in favour of refinements as corresponding to a very general notion of epistemic updates, just as the analogous results for bisimulations form a strong case in favour of bisimulations as corresponding to the notion of modal equivalence.

Of course, refinements are not {\em the most general} notion of epistemic updates, as for example, models of belief revision permit agents to forget or revise previous information~\cite{alchourron:1985}, likewise models of epistemic updates with awareness can cause agents to forget information~\cite{vanditmarsch:2012}, and such updates do not preserve positive formulas.
However refinements do generalise forms of epistemic updates such as public announcements~\cite{plaza:1989,gerbrandy:1997}, arrow updates~\cite{kooi:2011a} and action models~\cite{baltag:1999, baltag:2004}.
As action models generalise public announcements and arrow updates, we will consider the relationship between refinements and the results of action models.

van Ditmarsch and French~\cite{vanditmarsch:2009} showed that executing an action model results in a refinement.
We restate this result.

\begin{proposition}\label{action-models-refinements}
Let $\kPModelAndTuple{\kStateS}$ be a pointed Kripke model and let $\aPModelAndTuple{\aStateS}$ be an action model such that $\kPModel{\kStateS} \entails \aPrecondition(\aStateS)$.
Then $\kPModel{\kStateS} \simulates \kPModel{\kStateS} \exec \aPModel{\aStateS}$.
\end{proposition}

\begin{proof}
Let $\kPModel{\kStateS} \exec \aPModel{\aStateS} = \kPModelTupleP{(\kStateS, \aStateS)}$ where:
\begin{eqnarray*}
    \kStatesP &=& \{(\kStateT, \aStateT) \in \kStates \times \aStates \mid \kPModel{\kStateT} \entails \aPrecondition(\aStateT)\}\\
    \kAccessibilityPA &=& \{((\kStateT, \aStateT), (\kStateU, \aStateU)) \in \kStatesP \times \kStatesP \mid (\kStateT, \kStateU) \in \kAccessibilityA, (\aStateT, \aStateU) \in \aAccessibilityA\}\\
    \kValuationP(\atomP) &=& \{(\kStateT, \aStateT) \in \kStatesP \mid \kStateT \in \kValuation(\atomP)\}
\end{eqnarray*}
and let $\refinement \subseteq \kStates \times \kStatesP$ be a relation such that $(\kStateT, (\kStateT, \aStateT)) \in \refinement$ for every $(\kStateT, \aStateT) \in \kStatesP$.
We will show that $\refinement$ is a refinement and therefore $\kPModel{\kStateS} \simulates \kPModel{\kStateS} \exec \aPModel{\aStateS}$.
Let $\atomP \in \atoms$, $\agentA \in \agents$ and $(\kStateT, (\kStateT, \aStateT)) \in \refinement'$.
We show that the conditions {\bf atoms-$\atomP$} and {\bf back-$\agentA$} hold.

\paragraph{atoms-$\atomP$}
By construction, $\kStateT \in \kValuation(\atomP)$ if and only if $(\kStateT, \aStateT) \in \kValuationP(\atomP)$.

\paragraph{back-$\agentA$}
Let $(\kStateU, \aStateU) \in \kSuccessorsPA{(\kStateT, \aStateT)}$.
Then by construction $\kStateU \in \kSuccessorsA{\kStateT}$ and $(\kStateU, (\kStateU, \aStateU)) \in \refinement $.

Therefore $\refinement$ is a refinement and $\kPModel{\kStateS} \simulates \kPModel{\kStateS} \exec \aPModel{\aStateS}$.
\end{proof}

van Ditmarsch and French~\cite{vanditmarsch:2009} also showed that the refinements of a finite Kripke model are bisimilar to the results of executing an action model.
We show this result again for our more general definition of refinements.
Whereas van Ditmarsch and French~\cite{vanditmarsch:2009} showed this result using the common knowledge operator, our result is without the common knowledge operator.

\begin{proposition}
Let $\kPModel{\kStateS}$ be a finite Kripke model and let $\kPModelP{\kStateSP}$ be a (possibly infinite) Kripke model such that $\kPModel{\kStateS} \simulatesBs \kPModelP{\kStateSP}$.
Then there exists an action model $\aPModel{\aStateS}$ such that $\kPModel{\kStateS} \entails \aPrecondition(\aStateS)$ and $\kPModel{\kStateS} \exec \aPModel{\aStateS} \bisimilar \kPModelP{\kStateSP}$.
\end{proposition}

\begin{proof}
Without loss of generality assume that $\kPModel{\kStateS}$ is bisimulation contracted.
Let $\refinement \subseteq \kStates \times \kStatesP$ be a $\agentsB$-refinement from $\kPModel{\kStateS}$ to $\kPModelP{\kStateSP}$.

For every $\kStateT, \kStateU \in \kStates$ such that $\kStateT \neq \kStateU$, as $\kModel$ is bisimulation contracted then $\kPModel{\kStateT} \nbisimilar \kPModel{\kStateU}$, hence from Proposition~\ref{modal-hennessy-milner} there exists $\phi_{\kStateT, \kStateU} \in \langMl$ such that $\kPModel{\kStateT} \entails \phi_{\kStateT, \kStateU}$ but $\kPModel{\kStateU} \nentails \phi_{\kStateT, \kStateU}$.
For every $\kStateT \in \kStates$ let $\phi_{\kStateT} = \bigwedge_{\kStateU \in \kStates \setminus \{\kStateT\}} \phi_{\kStateT, \kStateU}$.
Then for every $\kStateT, \kStateU \in \kStates$ we have that $\kPModel{\kStateU} \entails \phi_{\kStateT}$ if and only if $\kStateU = \kStateT$.

We construct an action model $\aPModelAndTuple{\kStateSP}$ where:
\begin{eqnarray*}
    \aStates &=& \kStatesP\\
    \aAccessibilityA &=& \kAccessibilityPA\\
    \aPrecondition(\kStateTP) &=& \bigvee_{(\kStateT, \kStateTP) \in \refinement} \phi_{\kStateT}
\end{eqnarray*}

Let $\kModelAndTuplePP = \kModel \exec \aModel$. 
We note that $(\kStateT, \kStateTP) \in \kStatesPP$ if and only if $(\kStateT, \kStateTP) \in \refinement$.

Let $\bisimulation' \subseteq \kStatesP \times \kStatesPP$ such that $(\kStateTP, (\kStateT, \kStateTP)) \in \bisimulation' $ for every $(\kStateT, \kStateTP) \in \refinement$.
We will show that $\bisimulation'$ is a bisimulation.
Let $\atomP \in \atoms$, $\agentA \in \agents$, and $(\kStateTP, (\kStateT, \kStateTP)) \in \bisimulation'$.
We show that the conditions {\bf atoms-$\atomP$}, {\bf forth-$\agentA$} and {\bf back-$\agentA$} hold.

\paragraph{atoms-$\atomP$}
{\bf atoms-$\atomP$} follows directly from $(\kStateT, \kStateTP) \in \refinement$ and {\bf atoms-$\atomP$} for $\refinement$.

\paragraph{forth-$\agentA$}
Let $\kStateUP \in \kSuccessorsPA{\kStateTP}$.
As $(\kStateT, \kStateTP) \in \refinement$ from {\bf back-$\agentA$} for $\refinement$ there exists $\kStateU \in \kSuccessorsA{\kStateT}$ such that $(\kStateU, \kStateUP) \in \refinement$.
Therefore $(\kStateUP, (\kStateU, \kStateUP)) \in \bisimulation'$.

\paragraph{back-$\agentA$}
Let $(\kStateU, \kStateUP) \in \kSuccessorsPPA{(\kStateT, \kStateTP)}$.
Then by construction $\kStateUP \in \kSuccessorsPA{\kStateTP}$ and $(\kStateUP, (\kStateU, \kStateUP)) \in \bisimulation'$.

Therefore $\bisimulation'$ is a bisimulation.
In particular we note as $(\kStateS, \kStateSP) \in \refinement$ then $(\kStateSP, (\kStateS, \kStateSP)) \in \bisimulation'$ and so $\kPModel{\kStateS} \exec \aPModel{\aStateS} \bisimilar \kPModelP{\kStateSP}$.
\end{proof}

We note however that refinements of infinite Kripke models may not correspond to the result of executing any action model.

\begin{example}
Suppose that $\atoms = \naturals$ and $\agents = \{\agentA\}$.
Let $\kPModelAndTuple{\kStateS}$ and $\kPModelAndTuple{\kStateSP}$ be pointed Kripke models where:
\begin{eqnarray*}
    \kStates &=& \powerset(\naturals)\\
    \kAccessibilityA &=& \kStates^2\\
    \kValuation(n) &=& \{\kStateT \in \kStates \mid n \in \kStateT\}\\
    \kStateS &=& \emptyset
\end{eqnarray*}
and where:
\begin{eqnarray*}
    \kStatesP &=& \{\kStateTP \in \powerset(\naturals) \mid n \in \naturals \text{ such that } n \text{ is even}\}\\
    \kAccessibilityPA &=& \kStatesP^2\\
    \kValuationP(n) &=& \{\kStateTP \in \kStatesP \mid n \in \kStateTP\}\\
    \kStateSP &=& \emptyset
\end{eqnarray*}
We note that $\kPModel{\kStateS} \simulates \kPModelP{\kStateSP}$.
Let $\aPModelAndTuple{\aStateS}$ be a pointed action model such that $\kPModel{\kStateS} \entails \aPrecondition(\aStateS)$ and let $\kPModelPP{\kStateSPP} = \kPModel{\kStateS} \exec \aPModel{\aStateS}$.
Suppose that for every $\kStateT \in \kSuccessorsA{\kStateS}$, $\aStateT \in \aSuccessorsA{\aStateS}$ we have $\kPModel{\kStateT} \nentails \aPrecondition(\aStateT)$.
Then by the definition of action model execution we must have that $\kSuccessorsPPA{\kStateSPP} = \emptyset$ so $\kPModelP{\kStateSP} \not\bisimilar \kPModelPP{\kStateSPP}$.
Suppose that there exists $\kStateT \in \kSuccessorsA{\kStateS}$, $\aStateT \in \aSuccessorsA{\aStateS}$ such that $\kPModel{\kStateT} \entails \aPrecondition(\aStateT)$.
Let $\atomsQ$ be the set of propositional atoms appearing in $\aPrecondition(\aStateT)$.
Then $\atomsQ$ is finite and there exists an odd integer $m \in \naturals$ such that $m \notin \atomsQ$.
Let $\kStateU = \{m\} \cup \kStateT$.
As $m \notin \atomsQ$ then $\kPModel{\kStateU} \entails \aPrecondition(\aStateT)$.
Then by the definition of action model execution there exists $\kStateUPP \in \kSuccessorsPPA{\kStateSPP}$ such that $m \in \kValuationPP(\kStateUPP)$.
By construction as $m$ is odd there is no $\kStateUP \in \kSuccessorsPA{\kStateSP}$ such that $m \in \kValuationP(\kStateUP)$ so $\kPModelP{\kStateSP} \not\bisimilar \kPModelPP{\kStateSPP}$.
Therefore for every pointed action model $\aPModel{\aStateS}$ such that $\kPModel{\kStateS} \entails \aPrecondition(\aStateS)$ we have $\kPModel{\kStateS} \exec \aPModel{\aStateS} \not\bisimilar \kPModelP{\kStateSP}$.
\end{example}

Finally we note that, as with bisimulations, there is a unique, maximal refinement from one Kripke model to another, and it can be computed in polynomial time.

\begin{lemma}\label{refinement-union}
Let $\agentsB \subseteq \agents$ be a set of agents, let $\kModel$ and $\kModelP$ be Kripke models and let $\refinement, \refinement' \subseteq \kStates \times \kStatesP$ be $\agentsB$-refinements.
Then $\refinement \cup \refinement'$ is also a $\agentsB$-refinement.
\end{lemma}

\begin{proof}
This follows directly from the definition of a refinement, noting that the conditions {\bf atoms}, {\bf forth}, and {\bf back} for individual pairs in a relation are preserved under unions with other relations.
\end{proof}

\begin{proposition}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kModel$ and $\kModelP$ be Kripke models such that $\kModel \simulatesBs \kModelP$.
Then there is a unique, maximal $\agentsB$-refinement from $\kModel$ to $\kModelP$.
\end{proposition}

\begin{proof}
From Lemma~\ref{refinement-union} the union of all $\agentsB$-refinements from $\kModel$ to $\kModelP$ is a $\agentsB$-refinement, and so it is the unique, maximal $\agentsB$-refinement from $\kModel$ to $\kModelP$.
\end{proof}

\begin{proposition}
Let $\agentsB \subseteq \agents$ be a set of agents and let $\kModel$ and $\kModelP$ be finite Kripke models defined on a finite set of propositional atoms such that $\kModel \simulatesBs \kModelP$.
Then the maximal refinement from $\kModel$ to $\kModelP$ can be computed in polynomial time.
\end{proposition}

\begin{proof}
We compute the relation $\refinement_0 \subseteq \kStates \times \kStatesP$ such that $(\kStateS, \kStateSP) \in \refinement_0$ if and only if for every $\atomP \in \atoms$ the pair $(\kStateS, \kStateSP)$ satisfies the condition {\bf atoms-$\atomP$}.
The relation $\refinement_0$ can be computed in $O(||\kStates|| \times ||\kStatesP||)$ time.
Let $i \in \naturals$.
Given $\refinement_i$ we compute the relation $\refinement_{i+1} \subseteq \refinement_i$ such that $(\kStateS, \kStateSP) \in \refinement_{i+1}$ if and only if for every $\agentA \in \agents$ and $\agentC \in \agents \setminus \agentsB$ the pair $(\kStateS, \kStateSP)$ satisfies the conditions {\bf forth-$\agentC$} and {\bf back-$\agentA$}.
The relation $\refinement_{i+1}$ can be computed in $O(||\kStates|| \times ||\kStatesP|| \times ||\kAccessibilityRel|| \times ||\kAccessibilityRelP||)$ time.
We repeat this process until we reach a fixed point, called $\refinement_n$.
The process cannot be repeated more than $||\kStates|| \times ||\kStatesP||$ times, as $||\refinement_0|| \leq ||\kStates|| \times ||\kStatesP||$.
Therefore $\refinement_n$ can be computed in polynomial time.

For every $\atomP \in \atoms$, $\agentA \in \agents$, $\agentC \in \agents \setminus \agentsB$ the relation $\refinement_n$ satisfies the conditions {\bf atoms-$\atomP$}, {\bf forth-$\agentC$} and {\bf back-$\agentA$}.
Therefore if $\refinement_n$ is non-empty then it is a $\agentsB$-refinement.

Let $\refinement$ be the maximal $\agentsB$-refinement from $\kModel$ to $\kModelP$.
As $\refinement$ satisfies {\bf atoms-$\atomP$} for every $\atomP \in \atoms$ then $\refinement \subseteq \refinement_0$.
Let $i \in \naturals$ and suppose that $\refinement \subseteq \refinement_i$. 
We note that $\refinement \subseteq \refinement_{i + 1}$.
So $\refinement \subseteq \refinement_n$, and we have that $\refinement_n$ is non-empty and therefore $\refinement_n$ is a $\agentsB$-refinement.
As $\refinement$ is the maximal $\agentsB$-refinement then $\refinement_n \subseteq \refinement$.
Therefore $\refinement_n = \refinement$ and the above algorithm computes the maximal $\agentsB$-refinement.
\end{proof}
